from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import os
import platform
from pathlib import Path
from dotenv import load_dotenv
from langchain_community.llms import Ollama

load_dotenv()

app = FastAPI()

# CORS para o frontend aceitar
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://frontend:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str
    conversation_id: str = None


def is_running_in_docker() -> bool:
    """
    Deteta se o c√≥digo est√° a correr dentro de um container Docker.
    
    Usa m√∫ltiplas verifica√ß√µes para maior robustez:
    1. Exist√™ncia do ficheiro /.dockerenv (m√©todo mais comum)
    2. An√°lise do /proc/1/cgroup (processo init do container)
    3. Vari√°vel de ambiente DOCKER_CONTAINER (opcional)
    
    Returns:
        bool: True se estiver em Docker, False caso contr√°rio
    """
    
    # M√©todo 1: Verifica o ficheiro .dockerenv (mais simples e confi√°vel)
    if Path("/.dockerenv").exists():
        return True
    
    # M√©todo 2: Verifica o cgroup (funciona em mais casos)
    try:
        with open("/proc/1/cgroup", "rt") as f:
            content = f.read()
            # Verifica se cont√©m "docker" ou "containerd" no cgroup
            if "docker" in content or "containerd" in content:
                return True
    except Exception:
        # Se n√£o conseguir ler (ex: Windows), ignora
        pass
    
    # M√©todo 3: Vari√°vel de ambiente customizada (mais expl√≠cito)
    if os.getenv("DOCKER_CONTAINER") == "true":
        return True
    
    # M√©todo 4: Verifica se hostname parece ser de um container
    # Containers costumam ter hostnames hexadecimais curtos
    hostname = platform.node()
    if len(hostname) == 12 and all(c in "0123456789abcdef" for c in hostname):
        return True
    
    return False


def get_ollama_base_url() -> str:
    """
    Determina o URL base do Ollama baseado no ambiente.
    
    Prioridades:
    1. Vari√°vel de ambiente OLLAMA_BASE_URL (override manual)
    2. Se estiver em Docker -> host.docker.internal:11434
    3. Caso contr√°rio (desenvolvimento local) -> localhost:11434
    
    Returns:
        str: URL completo do Ollama
    """
    
    # Prioridade 1: Override manual via vari√°vel de ambiente
    env_url = os.getenv("OLLAMA_BASE_URL")
    if env_url:
        print(f"üìå [CONFIG] URL do Ollama via env: {env_url}")
        return env_url
    
    # Prioridade 2: Deteta automaticamente o ambiente
    in_docker = is_running_in_docker()
    
    if in_docker:
        url = "http://host.docker.internal:11434"
        print(f"üê≥ [DOCKER] Detetado ambiente Docker. URL Ollama: {url}")
        return url
    else:
        url = "http://localhost:11434"
        print(f"üíª [LOCAL] Detetado ambiente local. URL Ollama: {url}")
        return url


def get_environment_info() -> dict:
    """
    Retorna informa√ß√£o detalhada sobre o ambiente de execu√ß√£o.
    √ötil para debugging.
    """
    return {
        "running_in_docker": is_running_in_docker(),
        "platform": platform.system(),
        "hostname": platform.node(),
        "python_version": platform.python_version(),
        "ollama_url": get_ollama_base_url(),
        "dockerenv_exists": Path("/.dockerenv").exists(),
        "has_docker_env_var": os.getenv("DOCKER_CONTAINER") is not None,
    }

    
@app.post("/api/chat")
async def chat(request: ChatRequest):
    """Endpoint para chat com o Ollama"""
    ollama_url = get_ollama_base_url()
    
    try:
        llm = Ollama(
            model="gpt-oss:120b-cloud",
            base_url=ollama_url
        )
        response = llm.invoke(request.message)
        
        return {
            "response": response,
            "conversation_id": request.conversation_id
        }
    except Exception as e:
        print(f"‚ùå Erro ao conectar ao Ollama: {str(e)}")
        return {
            "error": f"Erro ao conectar ao Ollama: {str(e)}",
            "ollama_url": ollama_url,
            "suggestion": "Verifica se o Ollama est√° a correr e se OLLAMA_HOST=0.0.0.0:11434"
        }


@app.get("/api/crypto/{symbol}")
async def get_crypto_price(symbol: str):
    """Endpoint de exemplo para pre√ßos de crypto"""
    return {"symbol": symbol, "price": 50000}


@app.get("/")
async def root():
    """Health check b√°sico"""
    return {"status": "ok", "message": "Crypto API running"}


@app.get("/health")
async def health():
    """
    Health check detalhado com informa√ß√£o do ambiente.
    √ötil para debugging de problemas de conectividade.
    """
    return {
        "status": "healthy",
        "environment": get_environment_info()
    }


@app.get("/api/debug/environment")
async def debug_environment():
    """
    Endpoint de debug para ver toda a informa√ß√£o do ambiente.
    IMPORTANTE: Desativa isto em produ√ß√£o por seguran√ßa!
    """
    return get_environment_info()


# Log de inicializa√ß√£o
if __name__ == "__main__":
    import uvicorn
    
    print("\n" + "="*60)
    print("üöÄ A iniciar Crypto Intelligence API")
    print("="*60)
    
    env_info = get_environment_info()
    print(f"\nüìä Informa√ß√£o do Ambiente:")
    for key, value in env_info.items():
        print(f"   ‚Ä¢ {key}: {value}")
    
    print(f"\nüîó Ollama URL: {get_ollama_base_url()}")
    print("="*60 + "\n")
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True
    )